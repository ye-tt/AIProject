{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "652ebfcd-b330-424e-8ab7-35334672c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 如何设计模型 \n",
    "## 1. 考虑具体任务， 分类，回归； 图像，序列类的任务\n",
    "## 2. 文本 -> embedding  图像 -> 单通道/多通道  得到了可直接用于计算的feature_in\n",
    "##  feature_in  -> backbone -- > feature_in里提取出有效的feature \n",
    "##  图像  -- > CNN -- > stack CNN - > VGG network \n",
    "##  文本  -- > embedding -- > RNN 系列  ->  取最后一步的输出 \n",
    "## 3. output -> head\n",
    "##  分类 -> 1*10 logits -> softmax -> 概率\n",
    "##  多分类 -> 1* 10 logits -> 单个维度 做sigmoid / softmax -> \n",
    "##  回归 --> fc -> \n",
    "\n",
    "## 前向过程至此结束\n",
    "\n",
    "## 反向传播 -> loss  -> bp ->      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a82204-71fd-4c3f-8d9d-77c6d527f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 任务： 训练一个会做五言绝句的模型\n",
    "## 输出： 五言绝句序列  输入： 开头 \n",
    "## one/ many -- > many\n",
    "## RNN网络的场景\n",
    "\n",
    "## 文本输入  --> embedding  --> feature_in --- > extract feature (RNN/LSTM/GRU) --> （deep / bi） -> head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58b7f6a3-baa9-46d2-9e9e-b76e1faec0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# dir(Dataset) # __iter__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c7bb1-58ae-4a5b-b8c3-e5e1c32d2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# 0. 全局配置 (Config)\n",
    "# ==========================================\n",
    "class Config:\n",
    "    vocab_size = 20      # 词表大小 (0-19的数字)\n",
    "    seq_len = 10         # 序列长度\n",
    "    embed_dim = 32       # 词向量维度\n",
    "    hidden_dim = 64      # 隐层维度\n",
    "    batch_size = 16\n",
    "    lr = 0.001\n",
    "    epochs = 20\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ==========================================\n",
    "# 1. 数据模块 (共用)\n",
    "# ==========================================\n",
    "class SharedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    根据 task_mode 返回不同的 (input, target)\n",
    "    \"\"\"\n",
    "    def __init__(self, size=1000, mode='cls'):\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        # 随机生成数据: [size, seq_len]\n",
    "        self.data = torch.randint(0, Config.vocab_size, (size, Config.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]   #shape (size, Config.seq_len)\n",
    "\n",
    "        ### many -> one\n",
    "        if self.mode == 'cls':\n",
    "            # 任务：分类 (Classification)\n",
    "            # 规则：如果序列和为偶数，label=0，否则 label=1\n",
    "            label = 1 if x.sum().item() % 2 != 0 else 0\n",
    "            return x, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        ## many -> many\n",
    "        ### 我 爱 自 然 语 言 处 理\n",
    "        ## 爱 自 然 语 言 处 理  。\n",
    "        elif self.mode == 'gen':\n",
    "            # 任务：续写/生成 (Language Modeling)\n",
    "            # 规则：输入 [x1, x2, x3]，目标 [x2, x3, x4] (错位预测)\n",
    "            # 这里简单构造：Target 是 Input 循环左移一位\n",
    "            y = torch.roll(x, -1)\n",
    "            return x, y\n",
    "\n",
    "        ## many -> many \n",
    "        elif self.mode == 'trans':\n",
    "            # 任务：翻译 (Seq2Seq)\n",
    "            # 规则：模拟翻译，这里我们将序列 \"倒序\" 作为翻译目标\n",
    "            # Input: [1, 2, 3] -> Target: [3, 2, 1]\n",
    "            y = torch.flip(x, [0])\n",
    "            return x, y\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf626346-fe6f-4584-a35f-8947ef4dd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# help(nn.GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5a99e-4c38-4996-947c-5011eae710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # MLP 必须把整个序列展平 (Seq_Len * Embed_Dim)\n",
    "\n",
    "        ## torch构建网络\n",
    "        self.embedding = nn.Embedding(Config.vocab_size, Config.embed_dim)\n",
    "        \n",
    "        #经过embedding 后[batch, seq_len, embed_dim] -> 展平 -> [batch, seq_len * embed_dim]\n",
    "        self.fc = nn.Sequential(\n",
    "            ## W (Config.seq_len * Config.embed_dim, 128)\n",
    "            nn.Linear(Config.seq_len * Config.embed_dim, 128),  ## 全连接   --> batch * 128\n",
    "            nn.ReLU(), ## 激活\n",
    "            nn.Linear(128, 2) # 2类 ## 全连接  --> batch *2\n",
    "         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        embeds = self.embedding(x) # [batch, seq_len, embed]\n",
    "        # Flatten: MLP 无法处理变长序列，必须固定长度\n",
    "        ## reshape, 展平变成[batch, seq_len* embed]\n",
    "        flat = embeds.view(embeds.size(0), -1) \n",
    "        return self.fc(flat)  ## batch * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# RNN ->h_t+1=tanh(W_ih *x_t + b_ih + W_hh * h_t + b_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecffd8e-aa6b-4dda-924d-20921ef4c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 模型 B: RNN 分类器 (Many-to-One) ---\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(Config.vocab_size, Config.embed_dim)\n",
    "        # batch_first=True 让输入维度变为 [batch, seq, feature] \n",
    "        self.rnn = nn.RNN(Config.embed_dim, Config.hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(Config.hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: [batch, seq] \n",
    "        embeds = self.embedding(x)\n",
    "        #embeds.shape: [batch, seq,embed_dim] \n",
    "        # out: 每个时间步的输出, h_n: 最后一个时间步的隐状态\n",
    "        out, h_n = self.rnn(embeds) \n",
    "        # 分类任务通常只取最后一个时间步的隐状态 h_n    \n",
    "        # h_n shape: (num_layers × num_directions, batch_size, hidden_size) [1, batch, hidden] -> squeeze -> [batch, hidden]\n",
    "        last_hidden = h_n.squeeze(0)\n",
    "        return self.fc(last_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61f5b2f6-1fb7-4da2-a50a-8ab01e6ef0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 模型 C: RNN 生成器/续写 (Many-to-Many Synced) ---\n",
    "class RNNGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(Config.vocab_size, Config.embed_dim)\n",
    "        self.rnn = nn.RNN(Config.embed_dim, Config.hidden_dim, batch_first=True)\n",
    "        # 输出层要把隐状态映射回词表大小，预测下一个词\n",
    "        self.fc = nn.Linear(Config.hidden_dim, Config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        # out shape: [batch, seq_len, hidden]\n",
    "        out, _ = self.rnn(embeds)\n",
    "        # 对每一个时间步都进行预测\n",
    "        prediction = self.fc(out) # [batch, seq_len, vocab_size]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93a1dda-18c0-4a02-98f9-d4ba0b46bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 模型 D: Seq2Seq 翻译器 (Encoder-Decoder) ---\n",
    "class RNNTranslator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(Config.vocab_size, Config.embed_dim)\n",
    "        \n",
    "        # Encoder: 负责理解输入序列\n",
    "        self.encoder = nn.RNN(Config.embed_dim, Config.hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Decoder: 负责生成输出序列\n",
    "        self.decoder = nn.RNN(Config.embed_dim, Config.hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(Config.hidden_dim, Config.vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        # x: Source [batch, seq_len]\n",
    "        # target: Target [batch, seq_len] (训练时用于 Teacher Forcing)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 1. Encode\n",
    "        enc_embeds = self.embedding(x)\n",
    "        _, h_n = self.encoder(enc_embeds) # 获取 Encoder 最后的隐状态\n",
    "        \n",
    "        # 2. Decode\n",
    "        # 解码器的初始隐状态 = 编码器的最终隐状态 (Context Vector)\n",
    "        decoder_hidden = h_n\n",
    "        \n",
    "        # 教学演示简单起见，我们假设 Decoder 的第一个输入是全0或者特定的 Start Token\n",
    "        # 这里简单构造一个 Start Token (假设为0)\n",
    "        decoder_input = torch.zeros((batch_size, 1), dtype=torch.long).to(x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            dec_embed = self.embedding(decoder_input) # [batch, 1, embed]\n",
    "            \n",
    "            # 单步运行 Decoder\n",
    "            out, decoder_hidden = self.decoder(dec_embed, decoder_hidden)\n",
    "            step_out = self.fc(out) # [batch, 1, vocab_size]\n",
    "            outputs.append(step_out)\n",
    "            \n",
    "            # 决定下一个输入 (Teacher Forcing vs Autoregressive)\n",
    "            # 如果提供了 target (训练阶段)，有概率使用真实值作为下一步输入\n",
    "            if target is not None:\n",
    "                decoder_input = target[:, t].unsqueeze(1) # Teacher Forcing\n",
    "            else:\n",
    "                # 推理阶段：使用自己上一步预测最大概率的词\n",
    "                top1 = step_out.argmax(2)\n",
    "                decoder_input = top1\n",
    "                \n",
    "        # 拼接所有时间步的输出\n",
    "        outputs = torch.cat(outputs, dim=1) # [batch, seq_len, vocab_size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb93e5-b1ba-4748-b225-61755551611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, mode, epochs=10):\n",
    "    print(f\"\\n>>> 开始训练任务: [{mode}] | 模型: {model.__class__.__name__}\")\n",
    "    \n",
    "    # 构建数据\n",
    "    dataset = SharedDataset(size=1000, mode=mode)\n",
    "    loader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Config.lr) ## 参数更新的问题\n",
    "    model.to(Config.device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for x, y in loader:\n",
    "            x, y = x.to(Config.device), y.to(Config.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if mode == 'cls':\n",
    "                # 分类任务\n",
    "                outputs = model(x) # [batch, 2]\n",
    "                loss = criterion(outputs, y)\n",
    "                preds = outputs.argmax(dim=1) ## sofmatx -> array -> 取矩阵最大值的索引\n",
    "                correct += (preds == y).sum().item()\n",
    "                total_samples += y.size(0)\n",
    "                \n",
    "            elif mode == 'gen':\n",
    "                # 续写任务\n",
    "                outputs = model(x) # [batch, seq, vocab]\n",
    "                # CrossEntropy 需要 (N, C) 或 (N, C, d1...)，这里 reshape\n",
    "                # view() 是一个用于改变张量（Tensor）形状（shape） 的方法，类似于 NumPy 中的 reshape()\n",
    "                loss = criterion(outputs.view(-1, Config.vocab_size), y.view(-1))\n",
    "                \n",
    "            elif mode == 'trans':\n",
    "                # 翻译任务 (传入 y 用于 Teacher Forcing)\n",
    "                outputs = model(x, target=y) \n",
    "                loss = criterion(outputs.view(-1, Config.vocab_size), y.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # 打印日志\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            avg_loss = total_loss / len(loader)\n",
    "            if mode == 'cls':\n",
    "                acc = correct / total_samples\n",
    "                print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Acc = {acc:.2%}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472cbc9-9685-43fd-a635-2729b9c3d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. 分类任务 (Classification) ===\n",
      "\n",
      ">>> 开始训练任务: [cls] | 模型: MLPClassifier\n",
      "Epoch 5: Loss = 0.3208, Acc = 91.00%\n",
      "Epoch 10: Loss = 0.0259, Acc = 100.00%\n",
      "\n",
      ">>> 开始训练任务: [cls] | 模型: RNNClassifier\n",
      "Epoch 5: Loss = 0.6733, Acc = 58.40%\n",
      "Epoch 10: Loss = 0.6249, Acc = 65.70%\n",
      "\n",
      "[教学提示]: 对于固定长度的简单序列分类，MLP 和 RNN 都能做好。\n",
      "但如果序列长度变长，MLP 参数会爆炸且无法泛化到不同长度，而 RNN 参数共享，更适合序列。\n",
      "\n",
      "=== 2. 续写任务 (Generation) ===\n",
      "\n",
      ">>> 开始训练任务: [gen] | 模型: RNNGenerator\n",
      "Epoch 5: Loss = 2.9708\n",
      "Epoch 10: Loss = 2.9454\n",
      "Epoch 15: Loss = 2.9013\n",
      "Epoch 20: Loss = 2.8452\n",
      "Input:  [18  0 15 18 14 10 13  2 16 19]\n",
      "Target: [ 0 15 18 14 10 13  2 16 19 18] (左移)\n",
      "Pred:   [15 19  7 14 19  6 18 11  6 13]\n",
      "\n",
      "=== 3. 翻译任务 (Translation - Reverse Sequence) ===\n",
      "\n",
      ">>> 开始训练任务: [trans] | 模型: RNNTranslator\n",
      "Epoch 5: Loss = 2.2162\n",
      "Epoch 10: Loss = 1.7142\n",
      "Epoch 15: Loss = 1.4721\n",
      "Epoch 20: Loss = 1.3175\n",
      "Input:   [12  6 14  5  9 12  7 19 12  8]\n",
      "Target:  [ 8 12 19  7 12  9  5 14  6 12] (倒序)\n",
      "Decoder: [ 8  8 14  4  2  2  8 13  9 19]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 任务 1: 分类 (偶数和检测) ---\n",
    "    # 对比 MLP 和 RNN\n",
    "    print(\"=== 1. 分类任务 (Classification) ===\")\n",
    "    \n",
    "    # 训练 MLP\n",
    "    mlp = MLPClassifier()\n",
    "    train_model(mlp, mode='cls', epochs=10)\n",
    "    \n",
    "    # 训练 RNN\n",
    "    rnn_cls = RNNClassifier()\n",
    "    train_model(rnn_cls, mode='cls', epochs=10)\n",
    "    \n",
    "    print(\"\\n[教学提示]: 对于固定长度的简单序列分类，MLP 和 RNN 都能做好。\")\n",
    "    print(\"但如果序列长度变长，MLP 参数会爆炸且无法泛化到不同长度，而 RNN 参数共享，更适合序列。\")\n",
    "\n",
    "    # --- 任务 2: 续写 (Generation) ---\n",
    "    print(\"\\n=== 2. 续写任务 (Generation) ===\")\n",
    "    rnn_gen = RNNGenerator()\n",
    "    train_model(rnn_gen, mode='gen', epochs=20)\n",
    "    \n",
    "    # 测试一下效果\n",
    "     \n",
    "    pred_ids = pred.argmax(dim=2)\n",
    "    print(f\"Input:  {test_seq.cpu().numpy()[0]}\")\n",
    "    print(f\"Target: {np.roll(test_seq.cpu().numpy()[0], -1)} (左移)\")\n",
    "    print(f\"Pred:   {pred_ids.cpu().numpy()[0]}\")\n",
    "\n",
    "    # --- 任务 3: 翻译 (Translation/Seq2Seq) ---\n",
    "    print(\"\\n=== 3. 翻译任务 (Translation - Reverse Sequence) ===\")\n",
    "    rnn_trans = RNNTranslator()\n",
    "    train_model(rnn_trans, mode='trans', epochs=20)\n",
    "    \n",
    "    # 测试一下效果\n",
    "    test_seq = torch.randint(0, Config.vocab_size, (1, Config.seq_len)).to(Config.device)\n",
    "    # 推理时 target=None，不使用 Teacher Forcing\n",
    "    pred = rnn_trans(test_seq, target=None) \n",
    "    pred_ids = pred.argmax(dim=2)\n",
    "    print(f\"Input:   {test_seq.cpu().numpy()[0]}\")\n",
    "    print(f\"Target:  {test_seq.cpu().numpy()[0][::-1]} (倒序)\")\n",
    "    print(f\"Decoder: {pred_ids.cpu().numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0eb962f-3de6-4fa7-a523-4e5efe7e661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['首春:寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\\n', '初晴落景:晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\\n', '初夏:一朝春夏改，隔夜鸟花迁。阴阳深浅叶，晓夕重轻烟。哢莺犹响殿，横丝正网天。珮高兰影接，绶细草纹连。碧鳞惊棹侧，玄燕舞檐前。何必汾阳处，始复有山泉。\\n', '度秋:夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\\n', '仪鸾殿早秋:寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\\n', '秋日即目:爽气浮丹阙，秋光澹紫宫。衣碎荷疏影，花明菊点丛。袍轻低草露，盖侧舞松风。散岫飘云叶，迷路飞烟鸿。砌冷兰凋佩，闺寒树陨桐。别鹤栖琴里，离猿啼峡中。落野飞星箭，弦虚半月弓。芳菲夕雾起，暮色满房栊。\\n', '山阁晚秋:山亭秋色满，岩牖凉风度。疏兰尚染烟，残菊犹承露。古石衣新苔，新巢封古树。历览情无极，咫尺轮光暮。\\n', '帝京篇十首:秦川雄帝宅，函谷壮皇居。绮殿千寻起，离宫百雉余。连薨遥接汉，飞观迥凌虚。云日隐层阙，风烟出绮疏。岩廊罢机务，崇文聊驻辇。玉匣启龙图，金绳披凤篆。韦编断仍续，缥帙舒还卷。对此乃淹留，欹案观坟典。移步出词林，停舆欣武宴。雕弓写明月，骏马疑流电。惊雁落虚弦，啼猿悲急箭。阅赏诚多美，于兹乃忘倦。鸣笳临乐馆，眺听欢芳节。急管韵朱弦，清歌凝白雪。彩凤肃来仪，玄鹤纷成列。去兹郑卫声，雅音方可悦。芳辰追逸趣，禁苑信多奇。桥形通汉上，峰势接云危。烟霞交隐映，花鸟自参差。何如肆辙迹，万里赏瑶池。飞盖去芳园，兰桡游翠渚。萍间日彩乱，荷处香风举。桂楫满中川，弦歌振长屿。岂必汾河曲，方为欢宴所。落日双阙昏，回舆九重暮。长烟散初碧，皎月澄轻素。搴幌玩琴书，开轩引云雾。斜汉耿层阁，清风摇玉树。欢乐难再逢，芳辰良可惜。玉酒泛云罍，兰殽陈绮席。千钟合尧禹，百兽谐金石。得志重寸阴，忘怀轻尺璧。建章欢赏夕，二八尽妖妍。罗绮昭阳殿，芬芳玳瑁筵。佩移星正动，扇掩月初圆。无劳上悬圃，即此对神仙。以兹游观极，悠然独长想。披卷览前踪，抚躬寻既往。望古茅茨约，瞻今兰殿广。人道恶高危，虚心戒盈荡。奉天竭诚敬，临民思惠养。纳善察忠谏，明科慎刑赏。六五诚难继，四三非易仰。广待淳化敷，方嗣云亭响。\\n', '饮马长城窟行:塞外悲风切，交河冰已结。瀚海百重波，阴山千里雪。迥戍危烽火，层峦引高节。悠悠卷旆旌，饮马出长城。寒沙连骑迹，朔吹断边声。胡尘清玉塞，羌笛韵金钲。绝漠干戈戢，车徒振原隰。都尉反龙堆，将军旋马邑。扬麾氛雾静，纪石功名立。荒裔一戎衣，灵台凯歌入。\\n', '执契静三边:执契静三边，持衡临万姓。玉彩辉关烛，金华流日镜。无为宇宙清，有美璇玑正。皎佩星连景，飘衣云结庆。戢武耀七德，升文辉九功。烟波澄旧碧，尘火息前红。霜野韬莲剑，关城罢月弓。钱缀榆天合，新城柳塞空。花销葱岭雪，縠尽流沙雾。秋驾转兢怀，春冰弥轸虑。书绝龙庭羽，烽休凤穴戍。衣宵寝二难，食旰餐三惧。翦暴兴先废，除凶存昔亡。圆盖归天壤，方舆入地荒。孔海池京邑，双河沼帝乡。循躬思励己，抚俗愧时康。元首伫盐梅，股肱惟辅弼。羽贤崆岭四，翼圣襄城七。浇俗庶反淳，替文聊就质。已知隆至道，共欢区宇一。\\n']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. 数据准备与预处理\n",
    "# ==========================================\n",
    "\n",
    "with open('../poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_data = f.readlines()\n",
    "print(raw_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f495747-8b5a-4de8-a171-e57bda9ac005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4, 5, 6, 7, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# help(torch.roll)\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(torch.roll(x, -1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbf147-7172-4a81-9fbf-74921c47e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2385, -1.8266,  1.7584],\n",
      "        [ 0.6459, -0.5908,  0.8494],\n",
      "        [ 0.8723,  1.1961, -0.4590],\n",
      "        [ 1.7069, -0.3910,  0.9898],\n",
      "        [-1.6725, -0.5110, -0.2019],\n",
      "        [ 0.2438,  2.0117, -1.4535],\n",
      "        [ 0.6208,  1.1696, -0.6816],\n",
      "        [-0.5261, -0.9986, -0.9125],\n",
      "        [-0.1360,  0.1364, -0.5567],\n",
      "        [-1.6452,  0.2102,  0.2038]], requires_grad=True)\n",
      "tensor([[[ 0.6459, -0.5908,  0.8494]],\n",
      "\n",
      "        [[-1.6452,  0.2102,  0.2038]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# help(nn.Embedding)\n",
    "embedding = nn.Embedding(10, 3)\n",
    "print(embedding.weight)\n",
    "input = torch.LongTensor([[1], [9]])\n",
    "print(embedding(input)) #[1] 和 [9] 对应的embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33c6ceb7-6830-47ca-92d8-47ed3be23cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(nn.Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1cb2f-8a66-4e8b-8499-b2c91acb89e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "首春:寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\n",
      "\n",
      "初晴落景:晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\n",
      "\n",
      "初夏:一朝春夏改，隔夜鸟花迁。阴阳深浅叶，晓夕重轻烟。哢莺犹响殿，横丝正网天。珮高兰影接，绶细草纹连。碧鳞惊棹侧，玄燕舞檐前。何必汾阳处，始复有山泉。\n",
      "\n",
      "度秋:夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\n",
      "\n",
      "仪鸾殿早秋:寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\n",
      "\n",
      "秋日即目:爽气浮丹阙，秋光澹紫宫。衣碎荷疏影，花明菊点丛。袍轻低草露，盖侧舞松风。散岫飘云叶，迷路飞烟鸿。砌冷兰凋佩，闺寒树陨桐。别鹤栖琴里，离猿啼峡中。落野飞星箭，弦虚半月弓。芳菲夕雾起，暮色满房栊。\n",
      "\n",
      "山阁晚秋:山亭秋色满，岩牖凉风度。疏兰尚染烟，残菊犹承露。古石衣新苔，新巢封古树。历览情无极，咫尺轮光暮。\n",
      "\n",
      "帝京篇十首:秦川雄帝宅，函谷壮皇居。绮殿千寻起，离宫百雉余。连薨遥接汉，飞观迥凌虚。云日隐层阙，风烟出绮疏。岩廊罢机务，崇文聊驻辇。玉匣启龙图，金绳披凤篆。韦编断仍续，缥帙舒还卷。对此乃淹留，欹案观坟典。移步出词林，停舆欣武宴。雕弓写明月，骏马疑流电。惊雁落虚弦，啼猿悲急箭。阅赏诚多美，于兹乃忘倦。鸣笳临乐馆，眺听欢芳节。急管韵朱弦，清歌凝白雪。彩凤肃来仪，玄鹤纷成列。去兹郑卫声，雅音方可悦。芳辰追逸趣，禁苑信多奇。桥形通汉上，峰势接云危。烟霞交隐映，花鸟自参差。何如肆辙迹，万里赏瑶池。飞盖去芳园，兰桡游翠渚。萍间日彩乱，荷处香风举。桂楫满中川，弦歌振长屿。岂必汾河曲，方为欢宴所。落日双阙昏，回舆九重暮。长烟散初碧，皎月澄轻素。搴幌玩琴书，开轩引云雾。斜汉耿层阁，清风摇玉树。欢乐难再逢，芳辰良可惜。玉酒泛云罍，兰殽陈绮席。千钟合尧禹，百兽谐金石。得志重寸阴，忘怀轻尺璧。建章欢赏夕，二八尽妖妍。罗绮昭阳殿，芬芳玳瑁筵。佩移星正动，扇掩月初圆。无劳上悬圃，即此对神仙。以兹游观极，悠然独长想。披卷览前踪，抚躬寻既往。望古茅茨约，瞻今兰殿广。人道恶高危，虚心戒盈荡。奉天竭诚敬，临民思惠养。纳善察忠谏，明科慎刑赏。六五诚难继，四三非易仰。广待淳化敷，方嗣云亭响。\n",
      "\n",
      "饮马长城窟行:塞外悲风切，交河冰已结。瀚海百重波，阴山千里雪。迥戍危烽火，层峦引高节。悠悠卷旆旌，饮马出长城。寒沙连骑迹，朔吹断边声。胡尘清玉塞，羌笛韵金钲。绝漠干戈戢，车徒振原隰。都尉反龙堆，将军旋马邑。扬麾氛雾静，纪石功名立。荒裔一戎衣，灵台凯歌入。\n",
      "\n",
      "执契静三边:执契静三边，持衡临万姓。玉彩辉关烛，金华流日镜。无为宇宙清，有美璇玑正。皎佩星连景，飘衣云结庆。戢武耀七德，升文辉九功。烟波澄旧碧，尘火息前红。霜野韬莲剑，关城罢月弓。钱缀榆天合，新城柳塞空。花销葱岭雪，縠尽流沙雾。秋驾转兢怀，春冰弥轸虑。书绝龙庭羽，烽休凤穴戍。衣宵寝二难，食旰餐三惧。翦暴兴先废，除凶存昔亡。圆盖归天壤，方舆入地荒。孔海池京邑，双河沼帝乡。循躬思励己，抚俗愧时康。元首伫盐梅，股肱惟辅弼。羽贤崆岭四，翼圣襄城七。浇俗庶反淳，替文聊就质。已知隆至道，共欢区宇一。\n",
      "\n",
      "数据加载完毕，共 10 首诗，词表大小: 664\n",
      "\n",
      "开始训练 (Device: cpu)...\n",
      "Epoch 20/200 | Loss: 0.0038\n",
      "Epoch 40/200 | Loss: 0.0042\n",
      "Epoch 60/200 | Loss: 1.2497\n",
      "Epoch 80/200 | Loss: 0.0976\n",
      "Epoch 100/200 | Loss: 0.2292\n",
      "Epoch 120/200 | Loss: 0.2200\n",
      "Epoch 140/200 | Loss: 0.2066\n",
      "Epoch 160/200 | Loss: 0.3075\n",
      "Epoch 180/200 | Loss: 0.2538\n",
      "Epoch 200/200 | Loss: 0.2810\n",
      "\n",
      "=== 生成测试 ===\n",
      "开头 [寒随穷律] -> 生成: 寒随穷律。层必日危重起暮菊千思雪。广悠归悠苔园来仪声，扇\n",
      "开头 [晚霞聊自] -> 生成: 晚霞聊自花知阳露飞罢红悲空花衣起云风兰峡佩满云罍去异晴弥\n",
      "开头 [一朝春夏] -> 生成: 一朝春夏弓弓鸿机前林芳弦虚半月声志胡云梅淳红云钟弥华弥势\n",
      "开头 [夏律昨留] -> 生成: 夏律昨留戍澄荒里芳烽。日晃昭尚难烟疏空花阳河美，知案欢云\n",
      "\n",
      "=== 自由生成 (未见过的开头) ===\n",
      "开头 [春风] ->  春风烟起鸿出前月波虑旧渐竹疏绮树陨。飞白阳殿淳敬兰广\n"
     ]
    }
   ],
   "source": [
    "class TextPipeline:\n",
    "    def __init__(self, data_list):\n",
    "        # 1. 清洗数据：这里我们把标题去掉，只保留诗句内容，让模型专注于学写诗\n",
    "        # 也可以选择保留标题，看你想让模型学什么\n",
    "        self.sentences = []\n",
    "        for line in data_list[:10]:\n",
    "            if ':' in line:\n",
    "                print(line)\n",
    "                _, content = line.split(':') # 去掉 \"首春:\" 保留后面\n",
    "                self.sentences.append(content)\n",
    "            else:\n",
    "                self.sentences.append(line)\n",
    "        \n",
    "        # 2. 构建词表 (字 -> ID)\n",
    "        all_text = \"\".join(self.sentences)\n",
    "        self.chars = sorted(list(set(all_text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.idx2char = {i: c for i, c in enumerate(self.chars)}\n",
    "        \n",
    "        print(f\"数据加载完毕，共 {len(self.sentences)} 首诗，词表大小: {self.vocab_size}\")\n",
    "\n",
    "    def text_to_indices(self, text):\n",
    "        return [self.char2idx[c] for c in text if c in self.char2idx]\n",
    "\n",
    "    def indices_to_text(self, indices):\n",
    "        return \"\".join([self.idx2char[i] for i in indices])\n",
    "\n",
    "pipeline = TextPipeline(raw_data)\n",
    "\n",
    "# ==========================================\n",
    "# 2. 数据集构建 (Sliding Window)\n",
    "# ==========================================\n",
    "\n",
    "### 序列\n",
    "## x0 x1 x2 x3 x4 x5 x6 \n",
    "## 初始化一个h0\n",
    "## 0 时刻： h0, x0 -> RNN -> h1, o0  --> x1\n",
    "## 1 时刻  h1, x1 -> RNN -> h2, o1   --> x2\n",
    "## 2 时刻  h2, x2 -> RNN -> h3, o2   --> x3\n",
    "\n",
    "## torch.roll\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, pipeline, seq_len=6):\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # 滑动窗口构造数据\n",
    "        # 输入: \"寒随穷律变\" (len=5) -> 预测: \"，\"\n",
    "        ## 寒随穷律变，春逐鸟声开。\n",
    "        ## 1. 寒随穷律变 -> ,\n",
    "        ## 2. 随穷律变，-> 春\n",
    "        ## 3. 穷律变，春 -> 逐\n",
    "        ## 4. 律变，春逐 -> 鸟\n",
    "        ## 5. 变，春逐鸟 -> 声\n",
    "        ## 6. ，春逐鸟声 --> 开\n",
    "        ##  7. 春逐鸟声开 --> 。 --> End\n",
    "\n",
    "        ## 床前明月光，疑是地上霜\n",
    "        ## 1. 床 -> ,\n",
    "        ## 2.床前明月光 ->前明月光,\n",
    "        ## 6. 床前明月光，疑 --> 开\n",
    "\n",
    "        \n",
    "\n",
    "        ## 总结： 寒随穷律变  -- > 寒随穷律变，春逐鸟声开。\n",
    "        for sentence in pipeline.sentences:\n",
    "            indices = pipeline.text_to_indices(sentence)\n",
    "            for i in range(len(indices) - seq_len):\n",
    "                # x: 当前窗口的字\n",
    "                x_seq = indices[i : i + seq_len]\n",
    "                # y: 下一个字\n",
    "                y_char = indices[i + seq_len]\n",
    "                \n",
    "                self.inputs.append(x_seq)\n",
    "                self.targets.append(y_char)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "# 配置参数\n",
    "SEQ_LEN = 6   # 根据前6个字预测第7个字\n",
    "BATCH_SIZE = 4 # 数据很少，batch_size 设小一点\n",
    "HIDDEN_DIM = 128\n",
    "EMBED_DIM = 64\n",
    "EPOCHS = 200  # 数据少，需要多训练很多轮才能过拟合(记住)这些诗\n",
    "LR = 0.005\n",
    "\n",
    "dataset = PoetryDataset(pipeline, seq_len=SEQ_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 模型定义 (LSTM 生成模型)\n",
    "# ==========================================\n",
    "\n",
    "## 文本 -> 文本\n",
    "## 文本 -> embedding -> feature_in -> feature extractor (RNN) -> out \n",
    "\n",
    "class PoetryGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 注意：这里使用单向 LSTM，bidirectional=False\n",
    "        # 如果设为 True，模型无法用于生成任务（会看到未来）\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [batch, seq_len]\n",
    "        embeds = self.embedding(x)      # [batch, seq, embed]\n",
    "        out, hidden = self.lstm(embeds, hidden) # out: [batch, seq, hidden]\n",
    "        \n",
    "        # 我们只关心最后一个时间步的输出，用来预测下一个字\n",
    "        # out[:, -1, :] 取序列最后一个 timestep\n",
    "        last_output = out[:, -1, :] \n",
    "        \n",
    "        logits = self.fc(self.dropout(last_output)) # [batch, vocab]\n",
    "        return logits, hidden\n",
    "\n",
    "# ==========================================\n",
    "# 4. 训练循环\n",
    "# ==========================================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = PoetryGenerator(pipeline.vocab_size, EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"\\n开始训练 (Device: {device})...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # 训练时 hidden 设为 None，让它自动初始化为0\n",
    "\n",
    "        ## model -> __call__() -> forward\n",
    "        outputs, _ = model(x) \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. 生成/预测逻辑\n",
    "# ==========================================\n",
    "def generate_poem(model, start_text, length=40, temperature=0.8):\n",
    "    ## grad 关掉了\n",
    "    ##使用训练阶段累积的全局均值和方差（不更新）,关闭 Dropout，所有神经元都参与计算\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. 预处理输入\n",
    "    current_input = pipeline.text_to_indices(start_text)\n",
    "    # 如果输入长度不够，前面补随机或者补0 (这里简单处理，假设输入够长或不做padding)\n",
    "    # 实际应用中可以做 Padding，这里为了演示直接取最后 SEQ_LEN 个\n",
    "    if len(current_input) > SEQ_LEN:\n",
    "        current_input = current_input[-SEQ_LEN:]\n",
    "    \n",
    "    result = list(current_input)\n",
    "    \n",
    "    # 初始化 hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            # 把当前序列转为 tensor, 增加 batch 维度 -> [1, seq_len]\n",
    "            x = torch.tensor([current_input[-SEQ_LEN:]]).to(device)\n",
    "            \n",
    "            # 预测\n",
    "            logits, hidden = model(x, hidden)\n",
    "            \n",
    "            # --- 采样策略 (Temperature Sampling) ---\n",
    "            # temperature 越低，结果越保守（容易重复）；越高越随机\n",
    "            probs = torch.softmax(logits / temperature, dim=1)\n",
    "            \n",
    "            # 根据概率分布抽样\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # 记录结果\n",
    "            result.append(next_char_idx)\n",
    "            current_input.append(next_char_idx)\n",
    "\n",
    "            ## 训练的时候加入特殊标识 <EOS> -> 可以结束了\n",
    "            ## if next_char_idx == <EOS>: break\n",
    "            \n",
    "            \n",
    "            # 如果遇到句号，其实可以根据逻辑停止，这里为了展示让它生成固定长度\n",
    "\n",
    "    return pipeline.indices_to_text( )\n",
    "\n",
    "# ==========================================\n",
    "# 6. 测试效果\n",
    "# ==========================================\n",
    "print(\"\\n=== 生成测试 ===\")\n",
    "# 使用数据集里的开头来测试\n",
    "starts = [\"寒随穷律\", \"晚霞聊自\", \"一朝春夏\", \"夏律昨留\"]\n",
    "\n",
    "for s in starts:\n",
    "    poem = generate_poem(model, s, length=24, temperature=0.5) # 低温采样保证通顺\n",
    "    print(f\"开头 [{s}] -> 生成: {poem}\")\n",
    "\n",
    "print(\"\\n=== 自由生成 (未见过的开头) ===\")\n",
    "print(\"开头 [春风] -> \", generate_poem(model, \"春风\", length=24, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43752b8b-8978-40ca-9dbd-6d47e72794f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
