{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f605bcef",
   "metadata": {},
   "source": [
    "# 第一课 监督机器学习与非监督机器学习 \n",
    "## 1. 监督机器学习 Supervised Machine Leanring\n",
    "包含输入特征和对应的标签（正确答案）。\n",
    "例如：图片 + “猫”或“狗”的标签；邮件内容 + “垃圾邮件”或“非垃圾邮件”的标记。\n",
    "\n",
    "监督学习就像一个学生在做有标准答案的练习题。老师（标签）告诉学生每道题的正确答案，学生通过不断练习来学习规律，以便在考试（新数据）上答对\n",
    "### 1.1 回归\n",
    "预测连续数值。\n",
    "例：房价预测、股票价格预测\n",
    "### 1.2 分类\n",
    "将数据分到预定义的类别中。\n",
    "例：图像识别、情感分析。\n",
    "## 2. 无监督机器学习 Unsupervised Machine Leanring\n",
    "只包含输入特征，没有标签。\n",
    "例如：一组未分类的客户购买记录、一堆新闻文章。\n",
    "\n",
    "非监督学习则像一个学生面对一堆没有答案的谜题。他需要自己观察、分析，找出其中的规律，比如把相似的谜题归为一类，或者发现谜题背后的共同特征\n",
    "### 2.1 聚类\n",
    "将相似的数据点分组。\n",
    "例：客户细分、文档分组。\n",
    "### 2.2 降维\n",
    "减少数据的特征数量，保留重要信息。\n",
    "例：主成分分析（PCA）、t-SNE可视化。\n",
    "### 2.3关联规则学习\n",
    "发现变量之间的关系。\n",
    "例：购物篮分析，“买了A商品的人也常买B商品”。\n",
    "\n",
    "\n",
    "\n",
    "# 第二课\n",
    "## 1. **符号标准**\n",
    ">x: 输入变量或者输入特征\n",
    ">\n",
    ">y: 输出变量或目标变量\n",
    ">\n",
    ">m: 训练样本总数\n",
    ">\n",
    ">(x,y): 单个训练样本\n",
    ">\n",
    ">$(x^{(i)},y^{(i)})$: 训练集中的第i个训练样本\n",
    "\n",
    ">y-hat: 估值，预测值: $f_{wb}(x)=wx+b$\n",
    "\n",
    "## 2. 代价函数\n",
    "通过预测y hat 与目标值y 的差值， 即yhat-y\n",
    "平方误差代价函数：$J(_{wb})$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "## 3. 梯度函数\n",
    "\n",
    "梯度下降的目标是通过迭代的方式，最小化一个损失函数（Loss Function），从而找到使模型预测最准确的参数（如权重 w 和偏置 b）\n",
    "\n",
    "**核心思想**\n",
    "\n",
    "想象你站在一个山上的某个位置（初始参数），你的目标是走到山谷的最低点（全局最小值/最优参数）。由于你看不到整个地形，你只能感知脚下坡度的方向。梯度下降的策略是：沿着当前最陡峭的下坡方向（负梯度方向）走一小步，然后重复这个过程，直到到达谷底。\n",
    "\n",
    "* 梯度（Gradient）：是损失函数对各个参数的偏导数组成的向量。它指向函数值上升最快的方向。\n",
    "* 负梯度（-Gradient）：指向函数值下降最快的方向。\n",
    "* 学习率（Learning Rate, α）：决定你每一步走多大。太大可能 overshoot 最低点，太小则收敛慢\n",
    "\n",
    "* 更新规则： 参数 = 参数 - 学习率 × 损失函数对该参数的梯度\n",
    "\n",
    "**关键要点与注意事项**\n",
    "> * **学习率（Learning Rate)**：\n",
    ">   * 太大：可能导致震荡甚至发散（loss 越来越大）。\n",
    ">   * 太小：收敛速度非常慢。\n",
    ">   * 技巧：通常从 0.01, 0.001, 0.1 开始尝试，或使用学习率衰减。\n",
    "> * **特征缩放（Feature Scaling）**：\n",
    ">   *如果特征的尺度差异很大（如年龄 0-100，收入 0-1000000），梯度下降会走“之”字形，收敛慢。\n",
    ">   *解决方法：进行标准化（Standardization）或归一化（Normalization）。\n",
    "> * **局部最小值 vs 全局最小值**：\n",
    ">   *对于凸函数（如线性回归的 MSE），只有一个最小值，梯度下降总能找到。\n",
    ">   *对于非凸函数（如神经网络的损失函数），可能存在多个局部最小值，梯度下降可能陷入局部最优。\n",
    "> * **变体**：\n",
    ">   *批量梯度下降（Batch GD）：每次迭代使用所有训练数据计算梯度。稳定但慢。\n",
    ">   *随机梯度下降（SGD）：每次迭代只使用一个样本。快但震荡大。\n",
    ">   *小批量梯度下降（Mini-batch GD）：每次迭代使用一小批（如 32, 64, 128）样本。最常用，平衡了速度和稳定性。\n",
    "> * **收敛判断**：\n",
    ">   *可以设置最大迭代次数。\n",
    ">   *或者当损失函数的变化小于某个阈值时停止。\n",
    "\n",
    "## 4. 梯度函数\n",
    "$$ w = w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}$$\n",
    "$$ b = b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}$$\n",
    "\n",
    "$\\alpha$ 是学习率\n",
    "\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})$$\n",
    "\n",
    "## 5. 正规方程 VS. 梯度下降##\n",
    "\n",
    "正规方程和梯度下降是解决线性回归参数的常用方法。\n",
    "\n",
    "### 5.1. 正规方程 ###\n",
    "* 正规方程直接通过公式一次性计算出最优参数\n",
    "* 公式 $\\theta =(X^TX)^{-1}x^Ty$\n",
    "* 优点： 不需要选择学习率，计算速度快（样本量小，特征数不多的时候）\n",
    "* 缺点：当特征数量很大的时候，矩阵求逆计算量大，甚至不可行\n",
    "\n",
    "### 5.2. 梯度下降 ###\n",
    "* 通过不断迭代，逐步逼近最优参数\n",
    "* 需要设置学习率（step size），每次根据代价函数（损失函数）的梯度调整参数\n",
    "* 优点：适合大规模数据和高维特征，内存消耗小\n",
    "* 缺点： 需要调参（如学习率），可能陷入局部最优，收敛速度受参数影响\n",
    "\n",
    "# 第三课 \n",
    "## 1. 点积\n",
    "向量点积（Dot Product），也称为数量积（Scalar Product）或内积（Inner Product），是两个向量之间的一种基本运算，其结果是一个标量（即一个数）。\n",
    "\n",
    "<img style=\"height: 250px; width: 400px\" src=\"./C1_W2_Lab04_dot_notrans.gif\" width=800> \n",
    "\n",
    "## 2. 向量化\n",
    "\n",
    "<img style=\"height: 250px; width: 350px\" src=\"./xiangliang.png\" width=800> \n",
    "\n",
    "## 3. 特征缩放##\n",
    "\n",
    "特征缩放（Feature Scaling）是机器学习中一个非常重要的预处理步骤，其主要目的是将不同特征（变量）的取值范围调整到一个相似的尺度上。这能帮助许多机器学习算法更快、更稳定地收敛，并提高模型的性能。\n",
    "\n",
    "**为什么需要特征缩放？** \n",
    "\n",
    "* **算法对尺度敏感**：许多机器学习算法（尤其是基于距离或梯度下降的算法）对特征的尺度非常敏感。\n",
    "> * **基于距离的算法**：如K-近邻（KNN）、支持向量机（SVM）、K-均值聚类等，它们计算样本间的距离。如果一个特征的取值范围远大于其他特征（\n",
    "如，一个特征是0-1，另一个是0-1000），那么这个大范围的特征将在距离计算中占据主导地位，导致模型忽略小范围特征的信息。\n",
    "> * **基于梯度下降的算法**：如线性回归、逻辑回归、神经网络等。如果特征尺度差异大，损失函数的等高线会变得非常“狭长”，导致梯度下降过程需要走很多“之”字形才能到达最优解，收敛速度极慢。\n",
    "* **提升模型性能**：特征缩放可以防止某些特征因数值过大而对模型产生不合理的主导作用，使模型能更公平地学习每个特征的贡献。\n",
    "* **正则化效果**：在使用L1/L2正则化的模型中，如果特征尺度不一，正则化项会对不同特征施加不均衡的惩罚。\n",
    "\n",
    "**常见的特征缩放方法**\n",
    "\n",
    "* **标准化 (Standardization / Z-score Normalization)**\n",
    "> * **公式**：z = (x - μ) / σ\n",
    "> * **说明**：将特征转换为均值为0，标准差为1的分布。μ是特征的均值，σ是特征的标准差。\n",
    "> * **优点**：\n",
    "> * * 不受异常值的极端影响（相对鲁棒）。\n",
    "> * * 结果无固定范围，但大部分值在[-3, 3]之间。\n",
    "> * * 假设数据近似服从正态分布时效果较好。\n",
    "> * **适用场景**：逻辑回归、线性回归、SVM、神经网络、主成分分析（PCA）等。\n",
    "\n",
    "* **归一化 (Normalization / Min-Max Scaling)**\n",
    "> * **公式**：x' = (x - x_min) / (x_max - x_min)\n",
    "> * **说明**：将特征缩放到一个固定的范围，通常是[0, 1]。x_min和x_max是特征的最小值和最大值。\n",
    "> * **优点**：\n",
    "> * * 结果有明确的边界[0, 1]。\n",
    "> * * 保留了原始数据的分布形状。\n",
    "> * **缺点**：\n",
    "> * * 对异常值非常敏感。如果数据中存在一个极大的异常值，会导致其他大部分数据被压缩到一个很小的区间内。\n",
    "> * 适用场景：图像处理（像素值通常缩放到[0,1]）、神经网络（激活函数如Sigmoid、Tanh输入在[0,1]或[-1,1]效果好）。\n",
    "\n",
    "**使用特征缩放的注意事项**\n",
    "\n",
    "* **仅对训练集拟合**：缩放器（如StandardScaler）应该只在训练集上进行fit（计算均值、标准差等），然后用这个拟合好的缩放器去transform训练集、验证集和测试集。这是为了防止信息泄露（data leakage）。\n",
    "* **并非所有算法都需要**：决策树、随机森林、梯度提升树（如XGBoost, LightGBM）等基于树的模型，其分裂过程只关心特征的相对大小和顺序，不关心具体的数值尺度，因此通常不需要特征缩放。\n",
    "* **目标变量通常不缩放**：特征缩放主要针对输入特征（X）。对于回归问题的输出（y），有时为了数值稳定性也会进行缩放，但在预测后需要将结果反向缩放回原始尺度。\n",
    "* **类别特征**：特征缩放通常只应用于数值型连续特征。类别特征（Categorical Features）需要先进行编码（如One-Hot Encoding）。\n",
    "\n",
    "**总结**\n",
    "\n",
    "特征缩放是机器学习流程中一个简单但至关重要的步骤。选择哪种缩放方法取决于你的数据分布、是否存在异常值以及你使用的具体算法。标准化和归一化是最常用的方法，而鲁棒缩放在处理含异常值的数据时表现出色。正确应用特征缩放可以显著提升模型的训练效率和最终性能。\n",
    "\n",
    "## 4. 学习率的选择\n",
    "\n",
    "<img style=\"height: 280px; width: 600px\" src=\"./learningRate.png\"> \n",
    "\n",
    "### 4.1 如何选择学习率？\n",
    "* 经验性选择（手动设置）\n",
    "> * 常见的初始尝试值：0.1, 0.01, 0.001, 0.0001\n",
    "> * 从 0.01 或 0.001 开始尝试，观察损失函数的变化。\n",
    "> * 原则：选择一个能让损失平稳下降的最大学习率。\n",
    "\n",
    "## 5.特征工程\n",
    "\n",
    "\n",
    "\n",
    "# 第四课 分类\n",
    "\n",
    "分类（Classification） 是一种监督学习（Supervised Learning）任务，其目标是根据输入数据的特征，将其分配到预定义的类别或标签中。\n",
    "\n",
    "**什么是分类** \n",
    "\n",
    "简单来说，分类就是“判断属于哪一类”。分类是机器学习中最常见和实用的任务之一，广泛应用于：图像识别、文本分类、医疗诊断、金融风控、推荐系统\n",
    "\n",
    "<img style=\"height: 280px; width: 600px\" src=\"./fenlei.png\"> \n",
    "\n",
    "**分类的类型**\n",
    "\n",
    "* 二分类（Binary Classification）： 只有两个类别，是最常见的分类任务。例如：是/否、 正类/负类、 垃圾/非垃圾\n",
    "* 多分类（Multiclass Classification）： 有三个或更多类别，但每个样本只能属于一个类别。例如：识别手写数字（0～9），每个图像只能是其中一个数字。\n",
    "* 多标签分类（Multilabel Classification）： 每个样本可以同时属于多个类别。例如：一张图片可能同时包含“猫”、“狗”、“户外”。\n",
    "\n",
    "**常见的分类算法**\n",
    "\n",
    "* 逻辑回归（Logistic Regression）：\t特点：简单、可解释性强，输出概率，\t适用于二分类、特征线性可分\n",
    "* 支持向量机（SVM）\t\t特点：在高维空间表现好，适合小样本，适用于文本分类、图像识别\n",
    "* 决策树（Decision Tree）\t\t特点：易于理解和可视化，\t适用于规则清晰的分类问题\n",
    "* 随机森林（Random Forest）\t\t特点：集成方法，抗过拟合，精度高，\t适用于通用分类任务\n",
    "* 梯度提升树（如 XGBoost, LightGBM）\t\t特点：高精度，常用于竞赛，\t适用于结构化数据分类\n",
    "* K-近邻（KNN）\t\t特点：简单直观，基于相似度，\t适用于小数据集、低维数据\n",
    "\n",
    "## 1. 逻辑回归 Logistic Regression\n",
    "\n",
    "### 1.1 Sigmoid or Logistic Function\n",
    "\n",
    "**Sigmoid 函数公式**：\n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "**逻辑回归**\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "<img src=\"./C1_W3_LogisticRegression_right.png\"    style=\" width:300px;\" >\n",
    "\n",
    "\n",
    "### 1.2 决策边界\n",
    "在逻辑回归（Logistic Regression）中，决策边界（Decision Boundary） 是一个非常重要的概念，它决定了模型如何将不同类别的样本分开。\n",
    "**决策边界** 是指在特征空间中，模型用来区分不同类别的分界线（或曲面）。决策边界不是由模型训练直接优化的目标，而是由模型学到的权重（系数）和阈值自然形成的。\n",
    "\n",
    "在逻辑回归中，它是使得模型预测概率为 0.5 的所有点的集合。\n",
    "\n",
    "* 当样本位于决策边界的一侧时，模型预测为 类别 1\n",
    "* 当样本位于另一侧时，模型预测为 类别 0\n",
    "\n",
    "**总结** \n",
    "> * 决策边界 是逻辑回归中用于分类的分界线。\n",
    "> * 它由方程 $w^Tx + b = 0$ 定义，是一条直线（或超平面）。\n",
    "> * 决策边界将特征空间划分为两个区域，分别对应两个类别。\n",
    "> * 虽然逻辑回归本身是线性的，但结合多项式特征可以拟合非线性边界。\n",
    "\n",
    "### 1.3 逻辑回归的代价函数 \n",
    "#### 1.3.1 逻辑回归的损失函数  Logistic Loss function\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ 是单数据点损失\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ 是预测模型, 其中 $y^{(i)}$ 是实际目标值.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ \n",
    "其中函数 $g$ 是 sigmoid 函数.\n",
    "\n",
    "#### 1.3.2 逻辑回归的代价函数  Cost Function\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] $$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) $$\n",
    "    \n",
    "*  where m is the number of training examples in the data set and:\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)}) \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\n",
    "\\end{align}\n",
    "$$\n",
    " \n",
    "#### 1.3.3 梯度下降实现\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
